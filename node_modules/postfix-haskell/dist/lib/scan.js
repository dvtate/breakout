"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.lex = exports.BlockToken = exports.NumberToken = exports.LexerToken = exports.ContainerType = exports.TokenType = void 0;
const numbers_1 = require("./numbers");
// This handles Context-free grammar
// TODO optimize, use regex, etc.
// TODO chars
/**
 * Lexical type for token
 */
var TokenType;
(function (TokenType) {
    TokenType[TokenType["String"] = 0] = "String";
    TokenType[TokenType["Number"] = 1] = "Number";
    TokenType[TokenType["ContainerOpen"] = 2] = "ContainerOpen";
    TokenType[TokenType["ContainerClose"] = 3] = "ContainerClose";
    TokenType[TokenType["Identifier"] = 4] = "Identifier";
    TokenType[TokenType["Block"] = 5] = "Block";
})(TokenType = exports.TokenType || (exports.TokenType = {}));
;
// Internal
var ContainerType;
(function (ContainerType) {
    ContainerType[ContainerType["Curly"] = 0] = "Curly";
    ContainerType[ContainerType["Bracket"] = 1] = "Bracket";
    ContainerType[ContainerType["Paren"] = 2] = "Paren";
})(ContainerType = exports.ContainerType || (exports.ContainerType = {}));
;
// Generic token
class LexerToken {
    constructor(token, type, position, file) {
        this.token = token;
        this.type = type;
        this.position = position;
        this.file = file;
    }
}
exports.LexerToken = LexerToken;
LexerToken.Type = TokenType;
;
// Number literal
class NumberToken extends LexerToken {
    constructor(token, position, file) {
        super(token, TokenType.Number, position, file);
        this.value = new numbers_1.default().fromString(token);
    }
}
exports.NumberToken = NumberToken;
;
/**
 * The Token is initialized as a container open/close token
 * later it's converted to a block and the body is assgined
 */
class BlockToken extends LexerToken {
    constructor(token, position, file) {
        const type = '[{('.includes(token) ? TokenType.ContainerOpen : TokenType.ContainerClose;
        super(token, type, position, file);
        this.subtype = [
            ContainerType.Curly, ContainerType.Curly,
            ContainerType.Bracket, ContainerType.Bracket,
            ContainerType.Paren, ContainerType.Paren,
        ]['{}[]()'.indexOf(token)];
    }
}
exports.BlockToken = BlockToken;
;
/**
 * Describes tokenized string
 *
 * @param token - lexical token string
 * @param options - override/extend defaults
 */
function toToken(token, position, file) {
    // Trim whitespace
    token = token.trim();
    // Filter empty tokens
    if (token.length === 0)
        return null;
    // String
    if (token[0] === '"')
        return new LexerToken(token.substr(1, token.length - 2), TokenType.String, position, file);
    // Number (note this makes NaN an identifier)
    if (!isNaN(parseFloat(token)))
        return new NumberToken(token, position, file);
    // Separators
    if ('{}[]()'.includes(token))
        return new BlockToken(token, position, file);
    // Identifier
    return new LexerToken(token, TokenType.Identifier, position, file);
}
/**
 * Generates a list of tokens from given program source
 *
 * @param src - program source code
 * @param file - file name/path
 * @returns - List of tokens
 */
function lex(src, file) {
    let i = 0, prev = 0;
    const ret = [];
    // Add token to return
    const addToken = (s) => ret.push(toToken(s, i, file));
    // Find end of string
    const endStr = () => {
        // Determine if quote is escaped
        function isEscaped(s, i) {
            let e = false;
            while (s[--i] === '\\')
                e = !e;
            return e;
        }
        // Find end of string
        while (++i < src.length)
            if (src[i] === '"' && !isEscaped(src, i))
                break;
    };
    // For each char...
    while (i < src.length) {
        // Separator
        if ('[]{}()'.includes(src[i])) {
            addToken(src.substring(prev, i));
            addToken(src[i]);
            // Line-Comment
        }
        else if (src[i] === '#') {
            addToken(src.substring(prev, i));
            while (i < src.length && src[i] !== '\n')
                i++;
            // End of token
        }
        else if ([' ', '\t', '\n'].includes(src[i])) {
            addToken(src.substring(prev, i));
            // String literal
        }
        else if (['"', "'"].includes(src[i])) {
            addToken(src.substring(prev, i));
            prev = i;
            endStr();
            i++;
            addToken(src.substring(prev, i));
            // Middle of a token
        }
        else {
            i++;
            continue;
        }
        // Next character in token
        prev = ++i;
    }
    // EOF is a separator
    if (i !== prev)
        addToken(src.substring(prev, i));
    // Return list of token objects
    return ret.filter(Boolean);
}
exports.lex = lex;
;
/**
 * Throw syntax error
 *
 * @param message - reason for error
 * @param tokens - problematic tokens
 * @param file - file name/path
 */
function throwParseError(message, tokens, file) {
    // TODO convert to constructor
    throw {
        type: 'SyntaxError',
        message,
        tokens,
        stack: new Error(),
        file,
    };
}
/**
 * Generates a parse tree from list of tokens
 * Really only benefit here is that this collapses containers
 *
 * @param code - code to scan
 * @param file - file name/path
 */
function scan(code, file) {
    const tokens = lex(code, file);
    let ret = [];
    // Parse tokens
    tokens.forEach(tok => {
        switch (tok.type) {
            // Collapse containers
            case TokenType.ContainerClose:
                // Index of most recent container
                // TODO use .reverse+findIndex?
                let ind;
                for (ind = ret.length - 1; ind >= 0; ind--)
                    if (ret[ind].type === TokenType.ContainerOpen)
                        break;
                // No openers
                if (ind === -1)
                    throwParseError('Unexpected symbol ' + tok.token, [tok]);
                // Not matching
                // @ts-ignore
                if (ret[ind].subtype !== tok.subtype)
                    throwParseError(`Container mismatch ${ret[ind].token} vs. ${tok.token}`, [ret[ind], tok]);
                // Collapse body
                // Note for now only containers are for 'Block'
                ret[ind].body = ret.splice(ind + 1);
                ret[ind].type = TokenType.Block;
                break;
            // Handle basic tokens
            default:
                ret.push(tok);
                break;
        }
    });
    return ret;
}
exports.default = scan;
;
//# sourceMappingURL=scan.js.map